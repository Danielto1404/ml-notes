*ML Notes*

Documentation

1. [Frameworks](#frameworks)
2. [Transformers](#transformers)
3. [Detection](#detection)
4. [Normalization layers](#normalization-layers)
5. [Optimizers](#optimizers)

#### Frameworks:

* [PyTorch](https://pytorch.org/)
* [Numpy](https://numpy.org/)

#### Transformers:

* [Transformer](https://arxiv.org/abs/1706.03762): _Attention Is All You Need_
* [Reformer](https://arxiv.org/abs/2001.04451): _The Efficient Transformer (Local Attention with Hash / Sphere)_
* [Linformer](https://arxiv.org/abs/2006.04768): _Self-Attention with Linear Complexity (`O(n^2)` => `O(n)`)_
* [Vision Transformer](https://arxiv.org/abs/2010.11929): _An Image is Worth 16x16 Words: Transformers for Image
  Recognition at Scale_
* [DeiT](https://arxiv.org/abs/2012.12877): _Training data-efficient image transformers & distillation through
  attention: Distill token and huge CNN teachers_
* [CaiT](https://arxiv.org/abs/2103.17239): _Going deeper with Image Transformers_

#### Detection

* [DETR](https://arxiv.org/abs/2005.12872): _End-to-End Object Detection with Transformers_

#### Residual Networks:

* [RevNet](https://arxiv.org/abs/1707.04585): _The Reversible Residual Network â€“ Backpropagation Without Storing
  Activations_

#### Language models:

* [PEER](https://arxiv.org/pdf/2208.11663.pdf): _**P**lan, **E**dit, **E**xplain, **R**epeat. A collaborative language
  model from **Meta AI**_

#### Normalization layers:

* [Overview](https://theaisummer.com/normalization/)

<img alt="normalization" src="assets/norm_layers.png" style="border-radius: 8px">

#### Optimizers:

* [Adan](https://arxiv.org/abs/2208.06677): _Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models_