*ML Notes*

Documentation

1. [Frameworks](#frameworks)
2. [Transformers](#transformers)
3. [Normalization layers](#normalization-layers)

#### Frameworks:

* [PyTorch](https://pytorch.org/)
* [Numpy](https://numpy.org/)

#### Transformers:

* [Transformer](https://arxiv.org/abs/1706.03762): _Attention Is All You Need_
* [Reformer](https://arxiv.org/abs/2001.04451): _The Efficient Transformer (Local Attention with Hash / Sphere)_
* [Linformer](https://arxiv.org/abs/2006.04768): _Self-Attention with Linear Complexity (`O(n^2)` => `O(n)`)_
* [Vision Transformer](https://arxiv.org/abs/2010.11929): _An Image is Worth 16x16 Words: Transformers for Image
  Recognition at Scale_
* [DEiT](https://arxiv.org/abs/2012.12877): _Training data-efficient image transformers & distillation through
  attention: Distill token_

#### Residual Networks:

* [RevNet](https://arxiv.org/abs/1707.04585): _The Reversible Residual Network â€“ Backpropagation Without Storing
  Activations_

#### Normalization layers:

* [Overview](https://theaisummer.com/normalization/)

<img alt="normalization" src="assets/norm_layers.png" style="border-radius: 8px">
